{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Romaindujardin/SPOT/blob/main/Pipeline_SPOT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjbM1GrN_-Xo"
      },
      "source": [
        "# SPOT\n",
        "\n",
        "SPOT est un mini-système de projet de cours visant à simuler un appel automatique en classe via reconnaissance faciale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWLEnwHRF4Ul"
      },
      "source": [
        "## 1. Importation & installation des bibliothèques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhKrf6dtFzRQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# OpenCV contrib (LBPH) — on force une install propre (évite les conflits opencv-python/opencv-contrib)\n",
        "%pip -q uninstall -y opencv-python opencv-python-headless opencv-contrib-python opencv-contrib-python-headless\n",
        "%pip -q install --upgrade pillow-heif opencv-contrib-python-headless\n",
        "\n",
        "import cv2\n",
        "print(\"OpenCV version:\", cv2.__version__)\n",
        "print(\"cv2.face disponible:\", hasattr(cv2, \"face\") and hasattr(cv2.face, \"LBPHFaceRecognizer_create\"))\n",
        "\n",
        "if not (hasattr(cv2, \"face\") and hasattr(cv2.face, \"LBPHFaceRecognizer_create\")):\n",
        "    raise RuntimeError(\n",
        "        \"cv2.face est toujours indisponible.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0yY9aYzCKYP"
      },
      "source": [
        "### Définition de la structure classes et professeurs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLWd6rQfl4fb"
      },
      "source": [
        "```\n",
        "Eleve 1/\n",
        "        Photo1.png\n",
        "        Photo2.png\n",
        "Eleve 2/\n",
        "        Photo1.png\n",
        "        Photo2.png\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJonhx6PmTkh"
      },
      "source": [
        "Dans notre scénario, quand le prof arrive en cours, il scan sa carte d'école qui va ensuite lui permettre de choisir sa classe.\n",
        "\n",
        "Dans le dataset chaque professeur a plusieurs classes, chaque classe a plusieurs élèves et chaque élève a ses images enrigistrés pour entrainer le modèle.\n",
        "\n",
        "Ici on doit sélectionner le professeur et la classe pour avoir les bons élèves durant le cours et cliquer sur charger la classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usZb1vv89AxM"
      },
      "outputs": [],
      "source": [
        "# On initialise la variable ici pour qu'elle existe même avant le clic\n",
        "global ELEVES_ATTENDUS\n",
        "ELEVES_ATTENDUS = []\n",
        "\n",
        "# Structure\n",
        "ecole_structure = {\n",
        "    \"Professeur A\": {\n",
        "        \"IA\": [\"romain\", \"laurence\", \"chantal\", \"sophie\", \"grégory\", \"nathan\"],\n",
        "        \"Cyber\": [\"Eleve_Cy_1\", \"Eleve_Cy_2\", \"Eleve_Cy_3\", \"Eleve_Cy_4\", \"Eleve_Cy_5\"]\n",
        "    },\n",
        "    \"Professeur B\": {\n",
        "        \"IA\": [\"Eleve_IA_1\", \"Eleve_IA_2\", \"Eleve_IA_3\", \"Eleve_IA_4\", \"Eleve_IA_5\"],\n",
        "        \"Cyber\": [\"Eleve_Cy_1\", \"Eleve_Cy_2\", \"Eleve_Cy_3\", \"Eleve_Cy_4\", \"Eleve_Cy_5\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\n--- SÉLECTION DU COURS ---\")\n",
        "prof_dropdown = widgets.Dropdown(options=ecole_structure.keys(), description='Prof:')\n",
        "classe_dropdown = widgets.Dropdown(description='Classe:')\n",
        "btn_select = widgets.Button(description=\"Charger la classe\", button_style='success')\n",
        "output = widgets.Output()\n",
        "\n",
        "# Mise à jour des listes déroulantes\n",
        "def update_classes(change):\n",
        "    classe_dropdown.options = ecole_structure[change.new].keys()\n",
        "prof_dropdown.observe(update_classes, names='value')\n",
        "classe_dropdown.options = ecole_structure[prof_dropdown.value].keys()\n",
        "\n",
        "def on_button_click(b):\n",
        "    global ELEVES_ATTENDUS\n",
        "    with output:\n",
        "        clear_output()\n",
        "        prof = prof_dropdown.value\n",
        "        classe = classe_dropdown.value\n",
        "\n",
        "        # On remplit la variable globale\n",
        "        ELEVES_ATTENDUS = ecole_structure[prof][classe]\n",
        "\n",
        "        print(f\"Cours : {prof} - Classe {classe}\")\n",
        "        print(f\"Élèves chargés : {ELEVES_ATTENDUS}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Vérif rapide (dataset WEBCAM)\n",
        "        DATASET_ROOT = \"/content/dataset_crop\"\n",
        "        if os.path.exists(DATASET_ROOT):\n",
        "            missings = [e for e in ELEVES_ATTENDUS if not os.path.exists(os.path.join(DATASET_ROOT, e))]\n",
        "            if not missings:\n",
        "                print(\"Tous les dossiers élèves sont présents (dataset webcam).\")\n",
        "            else:\n",
        "                print(f\"Dossiers manquants (à créer après) : {missings}\")\n",
        "        else:\n",
        "            print(\"INFO: dataset webcam pas encore créé. Utilise les boutons WEBCAM juste en dessous.\")\n",
        "\n",
        "btn_select.on_click(on_button_click)\n",
        "display(prof_dropdown, classe_dropdown, btn_select, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ne pas oublier de faire son choix et d'appuyer sur charger la classe avant de passer à l'étape d'après, sinon rien n'est défini)"
      ],
      "metadata": {
        "id": "2Kb7ADURttZH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRuCrQQ2GfY0"
      },
      "source": [
        "## 2. Création du dataset via WEBCAM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A présent au lieu de créer en amont le dataset, on le crée directement dans colab pour avoir les memes conditions de webcam.\n",
        "\n",
        "On applique le pré-traitement lors de la création du dataset, on sauvegarde tout en png et on applique un crop de visage pour conserver uniquement le visage et plus les arrières plans pour éviter que le modèle apprenne un arrière plan = un élève."
      ],
      "metadata": {
        "id": "bXdkUfqmshDu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTklciP0BETp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import base64\n",
        "import cv2\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Javascript, clear_output\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "# 1) Vérifs\n",
        "if 'ELEVES_ATTENDUS' not in globals() or not ELEVES_ATTENDUS:\n",
        "    print(\"Aucun élève chargé. il Faut d'abord: sélectionner Prof + Classe puis 'Charger la classe'.\")\n",
        "else:\n",
        "    print(f\"Élèves chargés: {ELEVES_ATTENDUS}\")\n",
        "\n",
        "DATASET_WEBCAM_ROOT = \"/content/dataset_crop\"\n",
        "os.makedirs(DATASET_WEBCAM_ROOT, exist_ok=True)\n",
        "print(\"Dataset webcam:\", DATASET_WEBCAM_ROOT)\n",
        "\n",
        "# 2) Caméra JS\n",
        "js_camera_dataset = \"\"\"\n",
        "    if (!document.getElementById('videoElement')) {\n",
        "        var video = document.createElement('video');\n",
        "        video.id = 'videoElement';\n",
        "        video.style.display = 'none';\n",
        "        document.body.appendChild(video);\n",
        "        var canvas = document.createElement('canvas');\n",
        "        canvas.id = 'canvasElement';\n",
        "        canvas.style.display = 'none';\n",
        "        document.body.appendChild(canvas);\n",
        "    }\n",
        "    async function stream_frame() {\n",
        "        var video = document.getElementById('videoElement');\n",
        "        var canvas = document.getElementById('canvasElement');\n",
        "        if (video.paused) {\n",
        "            var stream = await navigator.mediaDevices.getUserMedia({\n",
        "              video: {\n",
        "                width: {ideal: 1280},\n",
        "                height: {ideal: 720},\n",
        "                facingMode: 'user'\n",
        "              }\n",
        "            });\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "        }\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        return canvas.toDataURL('image/jpeg', 0.92);\n",
        "    }\n",
        "\"\"\"\n",
        "display(Javascript(js_camera_dataset))\n",
        "\n",
        "def js_to_image(js_reply):\n",
        "    image_bytes = base64.b64decode(js_reply.split(',')[1])\n",
        "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "    img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "    return img\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))\n",
        "\n",
        "# IMPORTANT: on sauvegarde directement ce que LBPH consomme (200x200 GRAY + CLAHE)\n",
        "# => training/live parfaitement alignés, distances beaucoup plus basses.\n",
        "clahe_ds = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "\n",
        "def capture_one_face_ready():\n",
        "    \"\"\"Capture 1 frame, détecte le plus grand visage, retourne face_ready (GRAY 200x200 + CLAHE).\"\"\"\n",
        "    js_reply = eval_js('stream_frame()')\n",
        "    frame = js_to_image(js_reply)\n",
        "    if frame is None:\n",
        "        return None, \"frame None\"\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5, minSize=(140, 140))\n",
        "    if len(faces) == 0:\n",
        "        return None, \"pas de visage détecté (approche-toi / lumière)\"\n",
        "\n",
        "    (x, y, w, h) = max(faces, key=lambda f: f[2] * f[3])\n",
        "    margin = max(10, int(0.15 * min(w, h)))\n",
        "    x0 = max(0, x - margin)\n",
        "    y0 = max(0, y - margin)\n",
        "    x1 = min(gray.shape[1], x + w + margin)\n",
        "    y1 = min(gray.shape[0], y + h + margin)\n",
        "\n",
        "    face_roi = gray[y0:y1, x0:x1]\n",
        "    if face_roi.shape[0] < 140 or face_roi.shape[1] < 140:\n",
        "        return None, \"visage trop petit (rapproche-toi)\"\n",
        "\n",
        "    face_resized = cv2.resize(face_roi, (200, 200))\n",
        "    face_ready = clahe_ds.apply(face_resized)\n",
        "    return face_ready, f\"ok {w}x{h}\"\n",
        "\n",
        "def save_crops_for_student(student: str, n: int, out_widget: widgets.Output):\n",
        "    student = student.strip()\n",
        "    os.makedirs(os.path.join(DATASET_WEBCAM_ROOT, student), exist_ok=True)\n",
        "\n",
        "    saved = 0\n",
        "    tries = 0\n",
        "    with out_widget:\n",
        "        clear_output()\n",
        "        print(f\"Capture {n} images pour: {student}\")\n",
        "        print(\"Conseil: regarde caméra, puis tourne légèrement gauche/droite, varie expression.\")\n",
        "\n",
        "    while saved < n and tries < n * 6:\n",
        "        tries += 1\n",
        "        face_ready, msg = capture_one_face_ready()\n",
        "        if face_ready is None:\n",
        "            continue\n",
        "\n",
        "        ts = int(time.time() * 1000)\n",
        "        # On sauvegarde en PNG (pas de perte JPEG) + déjà prétraité pour LBPH\n",
        "        path = os.path.join(DATASET_WEBCAM_ROOT, student, f\"webcam_{ts}.png\")\n",
        "        cv2.imwrite(path, face_ready)\n",
        "        saved += 1\n",
        "        time.sleep(0.08)\n",
        "\n",
        "        if saved % 5 == 0:\n",
        "            with out_widget:\n",
        "                clear_output()\n",
        "                print(f\"Capture {n} images pour: {student}\")\n",
        "                print(f\"Progress: {saved}/{n} (tentatives={tries})\")\n",
        "\n",
        "    with out_widget:\n",
        "        clear_output()\n",
        "        print(f\"Terminé: {saved}/{n} pour {student} (tentatives={tries})\")\n",
        "        print(f\"Dossier: {os.path.join(DATASET_WEBCAM_ROOT, student)}\")\n",
        "        print(\"Ensuite: relance la cellule d'entraînement LBPH.\")\n",
        "\n",
        "# 3) UI: un bouton par élève + slider nb\n",
        "n_slider = widgets.IntSlider(value=30, min=10, max=80, step=5, description='Nb:', continuous_update=False)\n",
        "out = widgets.Output()\n",
        "\n",
        "btns = []\n",
        "if 'ELEVES_ATTENDUS' in globals() and ELEVES_ATTENDUS:\n",
        "    for eleve in ELEVES_ATTENDUS:\n",
        "        b = widgets.Button(description=f\"{eleve}\", button_style='info')\n",
        "        def make_handler(name):\n",
        "            def _h(_):\n",
        "                save_crops_for_student(name, int(n_slider.value), out)\n",
        "            return _h\n",
        "        b.on_click(make_handler(eleve))\n",
        "        btns.append(b)\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(\"<h3>Création dataset WEBCAM (1 élève à la fois)</h3>\"),\n",
        "    n_slider,\n",
        "    widgets.HBox(btns) if btns else widgets.HTML(\"<b>Charge d'abord une classe.</b>\"),\n",
        "    out\n",
        "])\n",
        "display(ui)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuWT5eOECgXL"
      },
      "source": [
        "### Vérification du dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EV9y6GS5-Z3"
      },
      "source": [
        "### Installation des bibliothèques disponibles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw6nxrupA0CI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pillow_heif import register_heif_opener\n",
        "\n",
        "register_heif_opener()\n",
        "\n",
        "# Dataset final utilisé partout\n",
        "LOCAL_CROP_DIR = \"/content/dataset_crop\"\n",
        "\n",
        "print(\"dataset =\", LOCAL_CROP_DIR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_6vBKbACikA"
      },
      "source": [
        "## 3. Entrainement du modèle sur le dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour l'entrainement, on applique du data-augmentation pour augmenter notre dataset.\n",
        "\n",
        "On change quelques petits détails pour renforcer le modèle, un peu de flou ou autre."
      ],
      "metadata": {
        "id": "lCvNmm9cvTOD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOfruFZy9My-"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Chemin vers le dataset\n",
        "DATASET_ROOT = \"/content/dataset_crop\"\n",
        "\n",
        "print(\"Démarrage de l'entraînement LBPH (Direct en mémoire)...\")\n",
        "print(f\"Dataset utilisé pour l'entraînement : {DATASET_ROOT}\")\n",
        "\n",
        "# --- 1. PARAMÈTRES ROBUSTES (Rayon et Grille augmentés) ---\n",
        "# Radius=3 : Regarde des pixels plus éloignés (mieux pour les visages un peu flous)\n",
        "# Neighbors=12 : Prend plus de voisins en compte\n",
        "# Grid=10x10 : Découpe le visage en 100 petites zones (plus précis)\n",
        "recognizer = cv2.face.LBPHFaceRecognizer_create(\n",
        "    radius=3,\n",
        "    neighbors=12,\n",
        "    grid_x=10,\n",
        "    grid_y=10\n",
        ")\n",
        "\n",
        "# --- 2. FONCTION D'AUGMENTATION ---\n",
        "# En mode WEBCAM, les images sont déjà prétraitées (200x200 GRAY + CLAHE).\n",
        "# On évite les augmentations \"luminosité\" qui dégradent souvent LBPH.\n",
        "def augment(img):\n",
        "    \"\"\"Génère 2 variations (original + léger flou).\"\"\"\n",
        "    return [img, cv2.GaussianBlur(img, (3, 3), 0)]\n",
        "\n",
        "# --- 3. PRÉPARATION DU DATASET ---\n",
        "\n",
        "faces = []\n",
        "labels = []\n",
        "label_map = {}\n",
        "current_label = 0\n",
        "\n",
        "if os.path.exists(DATASET_ROOT):\n",
        "    # On trie pour garantir l'ordre\n",
        "    for eleve in sorted(os.listdir(DATASET_ROOT)):\n",
        "        eleve_path = os.path.join(DATASET_ROOT, eleve)\n",
        "\n",
        "        if not os.path.isdir(eleve_path): continue\n",
        "\n",
        "        label_map[current_label] = eleve\n",
        "        print(f\" - Traitement de : {eleve} (ID={current_label})\")\n",
        "\n",
        "        image_count = 0\n",
        "        for file in os.listdir(eleve_path):\n",
        "            if file.lower().endswith(('jpg', 'jpeg', 'png')):\n",
        "                img_path = os.path.join(eleve_path, file)\n",
        "\n",
        "                # Lecture\n",
        "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "                if img is None: continue\n",
        "\n",
        "                # A. REDIMENSIONNEMENT (sécurité)\n",
        "                if img.shape != (200, 200):\n",
        "                    img = cv2.resize(img, (200, 200))\n",
        "\n",
        "                # IMPORTANT: pas de CLAHE ici (déjà appliqué lors de la capture WEBCAM)\n",
        "\n",
        "                # B. AUGMENTATION\n",
        "                augmented_images = augment(img)\n",
        "\n",
        "                # D. AJOUT AU DATASET\n",
        "                for aug_img in augmented_images:\n",
        "                    faces.append(aug_img)\n",
        "                    labels.append(current_label)\n",
        "                    image_count += 1\n",
        "\n",
        "        print(f\"   -> {image_count} images générées pour {eleve}\")\n",
        "        current_label += 1\n",
        "\n",
        "    # --- 4. ENTRAÎNEMENT ---\n",
        "    if len(faces) > 0:\n",
        "        print(f\"\\nEntraînement du modèle sur {len(faces)} visages (Original + Augmenté)...\")\n",
        "        recognizer.train(faces, np.array(labels))\n",
        "        print(\"Modèle entraîné et prêt en mémoire !\")\n",
        "        print(f\"Mapping : {label_map}\")\n",
        "    else:\n",
        "        print(\"ERREUR : Aucune image trouvée.\")\n",
        "else:\n",
        "    print(\"ERREUR : Dossier dataset introuvable.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug"
      ],
      "metadata": {
        "id": "_u2FM-m8nuo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici l'objectif est de vérifier que le modèle reconnaît bien des images du dataset_crop et estime un seuil réaliste (LBPH peut facilement être > 100 selon la qualité) pour le live."
      ],
      "metadata": {
        "id": "Zg4sCscoyXCu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_Ut7DcsBETs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "assert 'recognizer' in globals(), \"recognizer introuvable: relance la cellule d'entraînement\"\n",
        "\n",
        "DATASET_EVAL_ROOT = LOCAL_CROP_DIR if 'LOCAL_CROP_DIR' in globals() else \"/content/dataset_crop\"\n",
        "print(\"Dataset eval:\", DATASET_EVAL_ROOT)\n",
        "\n",
        "# Dataset WEBCAM: images déjà prétraitées (200x200 GRAY + CLAHE)\n",
        "\n",
        "def preprocess_for_lbph(img_bgr_or_gray):\n",
        "    if len(img_bgr_or_gray.shape) == 3:\n",
        "        g = cv2.cvtColor(img_bgr_or_gray, cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        g = img_bgr_or_gray\n",
        "    if g.shape != (200, 200):\n",
        "        g = cv2.resize(g, (200, 200))\n",
        "    return g\n",
        "\n",
        "# mapping nom -> id (basé sur label_map courant)\n",
        "name_to_id = {v: k for k, v in label_map.items()} if 'label_map' in globals() else {}\n",
        "\n",
        "rows = []  # (expected_name, predicted_name, distance)\n",
        "\n",
        "for name in sorted(os.listdir(DATASET_EVAL_ROOT)):\n",
        "    p = os.path.join(DATASET_EVAL_ROOT, name)\n",
        "    if not os.path.isdir(p):\n",
        "        continue\n",
        "    files = [f for f in os.listdir(p) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
        "    if not files:\n",
        "        continue\n",
        "\n",
        "    # on échantillonne pour aller vite\n",
        "    sample = random.sample(files, k=min(12, len(files)))\n",
        "\n",
        "    for f in sample:\n",
        "        img = cv2.imread(os.path.join(p, f))\n",
        "        if img is None:\n",
        "            continue\n",
        "        face = preprocess_for_lbph(img)\n",
        "\n",
        "        pred_id, dist = recognizer.predict(face)\n",
        "        pred_name = label_map.get(pred_id, \"<?>\")\n",
        "        rows.append((name, pred_name, float(dist)))\n",
        "\n",
        "# Résultats\n",
        "if not rows:\n",
        "    print(\"Aucun échantillon lu. Vérifie DATASET_EVAL_ROOT.\")\n",
        "else:\n",
        "    total = len(rows)\n",
        "    correct = sum(1 for exp, pred, _ in rows if exp == pred)\n",
        "    acc = correct / total\n",
        "\n",
        "    dists_correct = [d for exp, pred, d in rows if exp == pred]\n",
        "    dists_wrong = [d for exp, pred, d in rows if exp != pred]\n",
        "\n",
        "    print(f\"Samples: {total} | Accuracy (sur samples du dataset_crop): {acc*100:.1f}%\")\n",
        "\n",
        "    if dists_correct:\n",
        "        q50 = float(np.quantile(dists_correct, 0.50))\n",
        "        q90 = float(np.quantile(dists_correct, 0.90))\n",
        "        q95 = float(np.quantile(dists_correct, 0.95))\n",
        "        mx = float(np.max(dists_correct))\n",
        "        print(\"Distances CORRECTES:\")\n",
        "        print(f\"- median={q50:.1f} | p90={q90:.1f} | p95={q95:.1f} | max={mx:.1f}\")\n",
        "        print(f\"Seuil conseillé (p95 + marge): {min(300, q95 + 10):.1f}\")\n",
        "    else:\n",
        "        print(\"Aucune prédiction correcte sur le dataset_crop -> problème de modèle/mapping/prétraitement.\")\n",
        "\n",
        "    if dists_wrong:\n",
        "        mnw = float(np.min(dists_wrong))\n",
        "        q10w = float(np.quantile(dists_wrong, 0.10))\n",
        "        print(\"Distances FAUSSES (plus bas = plus dangereux):\")\n",
        "        print(f\"- min={mnw:.1f} | p10={q10w:.1f}\")\n",
        "\n",
        "    # affiche quelques erreurs typiques\n",
        "    errors = [(e, p, d) for e, p, d in rows if e != p]\n",
        "    errors = sorted(errors, key=lambda t: t[2])[:10]\n",
        "    if errors:\n",
        "        print(\"\\nErreurs (les plus 'confiantes'):\")\n",
        "        for e, p, d in errors:\n",
        "            print(f\"- expected={e} predicted={p} dist={d:.1f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En plus du debug sur le dataset, on fais un premier petit test live pour juger du seuil optimal"
      ],
      "metadata": {
        "id": "0VMZuBQm0IdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import base64\n",
        "import cv2\n",
        "from IPython.display import Javascript, display\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "assert 'recognizer' in globals(), \"Lance l'entraînement d'abord\"\n",
        "assert 'label_map' in globals(), \"label_map introuvable\"\n",
        "\n",
        "# Injecte / ré-injecte la fonction JS stream_frame\n",
        "js_camera_calib = \"\"\"\n",
        "    if (!document.getElementById('videoElement')) {\n",
        "        var video = document.createElement('video');\n",
        "        video.id = 'videoElement';\n",
        "        video.style.display = 'none';\n",
        "        document.body.appendChild(video);\n",
        "        var canvas = document.createElement('canvas');\n",
        "        canvas.id = 'canvasElement';\n",
        "        canvas.style.display = 'none';\n",
        "        document.body.appendChild(canvas);\n",
        "    }\n",
        "    async function stream_frame() {\n",
        "        var video = document.getElementById('videoElement');\n",
        "        var canvas = document.getElementById('canvasElement');\n",
        "        if (video.paused) {\n",
        "            var stream = await navigator.mediaDevices.getUserMedia({\n",
        "              video: {\n",
        "                width: {ideal: 1280},\n",
        "                height: {ideal: 720},\n",
        "                facingMode: 'user'\n",
        "              }\n",
        "            });\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "        }\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        return canvas.toDataURL('image/jpeg', 0.92);\n",
        "    }\n",
        "\"\"\"\n",
        "display(Javascript(js_camera_calib))\n",
        "\n",
        "def js_to_image(js_reply):\n",
        "    image_bytes = base64.b64decode(js_reply.split(',')[1])\n",
        "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "    img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "    return img\n",
        "\n",
        "# Détecteur + CLAHE (comme le live)\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "\n",
        "N = 30\n",
        "print(f\"Calibration live: capture {N} frames (mets-toi comme pendant l'appel)\")\n",
        "\n",
        "dists = []\n",
        "tries = 0\n",
        "while len(dists) < N and tries < N * 15:\n",
        "    tries += 1\n",
        "    js_reply = eval_js('stream_frame()')\n",
        "    frame = js_to_image(js_reply)\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5, minSize=(140, 140))\n",
        "    if len(faces) == 0:\n",
        "        continue\n",
        "\n",
        "    (x, y, w, h) = max(faces, key=lambda f: f[2] * f[3])\n",
        "    margin = max(10, int(0.15 * min(w, h)))\n",
        "    x0 = max(0, x - margin)\n",
        "    y0 = max(0, y - margin)\n",
        "    x1 = min(gray.shape[1], x + w + margin)\n",
        "    y1 = min(gray.shape[0], y + h + margin)\n",
        "\n",
        "    face_roi = gray[y0:y1, x0:x1]\n",
        "    face_resized = cv2.resize(face_roi, (200, 200))\n",
        "    face_ready = clahe.apply(face_resized)\n",
        "\n",
        "    _, dist = recognizer.predict(face_ready)\n",
        "    dists.append(float(dist))\n",
        "    time.sleep(0.05)\n",
        "\n",
        "if not dists:\n",
        "    print(\"Aucun visage détecté pendant la calibration\")\n",
        "else:\n",
        "    d = np.array(dists)\n",
        "    p95 = float(np.quantile(d, 0.95))\n",
        "    reco = p95 + 10\n",
        "    print(f\"{len(dists)} samples | dist median={np.median(d):.1f} p90={np.quantile(d,0.9):.1f} p95={p95:.1f} max={np.max(d):.1f}\")\n",
        "    print(f\"Reco: mets SEUIL_DISTANCE ≈ p95 + 10 => {reco:.1f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "n7F4vIeHQ7Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mRkbIX9Cl-Y"
      },
      "source": [
        "## 4. Pipeline SPOT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTUVlYxrDODL"
      },
      "source": [
        "### Importation des bibliothèques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p56xvW78DNCH"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Javascript, clear_output\n",
        "from google.colab.output import eval_js\n",
        "import base64\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHYzfNjWC7ee"
      },
      "source": [
        "### Configuration des variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KetK-HkMDl1V"
      },
      "outputs": [],
      "source": [
        "# Configuration des variables\n",
        "BUFFER_TIME = 5.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW6CWT8MQLRJ"
      },
      "source": [
        "### Chargement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10P0f8k8DFu-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# On force un dataset de référence cohérent avec l'entraînement (crop visage)\n",
        "TRAIN_DATASET_ROOT = LOCAL_CROP_DIR if 'LOCAL_CROP_DIR' in globals() else \"/content/dataset_crop\"\n",
        "\n",
        "# On vérifie si le 'recognizer' existe déjà en mémoire (si tu viens de lancer l'étape 3)\n",
        "if 'recognizer' in globals():\n",
        "    print(\"Modèle détecté en mémoire (venant de l'entraînement). Pas besoin de recharger le fichier.\")\n",
        "else:\n",
        "    # Cas où tu redémarres le notebook le lendemain\n",
        "    print(\"Modèle non trouvé en mémoire. Tentative de chargement depuis le disque...\")\n",
        "    if os.path.exists(\"spot_lbph_model.yml\"):\n",
        "        recognizer = cv2.face.LBPHFaceRecognizer_create(\n",
        "            radius=3,\n",
        "            neighbors=12,\n",
        "            grid_x=10,\n",
        "            grid_y=10\n",
        "        )\n",
        "        recognizer.read(\"spot_lbph_model.yml\")\n",
        "        print(\"Modèle chargé depuis le fichier 'spot_lbph_model.yml'.\")\n",
        "    else:\n",
        "        print(\"ERREUR CRITIQUE : Le fichier modèle n'existe pas. Relance l'Étape 3 (Entraînement) !\")\n",
        "\n",
        "# Mapping des labels (doit matcher le dataset utilisé pour l'entraînement)\n",
        "if os.path.exists(TRAIN_DATASET_ROOT):\n",
        "    label_map = {i: name for i, name in enumerate(sorted(os.listdir(TRAIN_DATASET_ROOT)))}\n",
        "    print(f\"Classes chargées : {label_map}\")\n",
        "else:\n",
        "    print(f\"Dataset de référence introuvable : {TRAIN_DATASET_ROOT} (le mapping peut être faux)\")\n",
        "\n",
        "# Seuil unique (LBPH: plus bas = plus strict)\n",
        "SEUIL_DISTANCE = 175\n",
        "print(f\"Seuil de distance configuré à : {SEUIL_DISTANCE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SExY_r6ZHWXQ"
      },
      "source": [
        "### Interface et lancement du cours\n",
        "\n",
        "C'est ici que l'on voit le résultat, il faut se placer devant la caméra après avoir entrainé le modèle.\n",
        "\n",
        "Il va alors détecter les personnes présente, absente etc.\n",
        "\n",
        "Chaque personne est décompté en temps.\n",
        "\n",
        "Les données sont ensuite stocké pour envoyer au rapport du cours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-cOQ87fHLJN"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Javascript, clear_output\n",
        "from google.colab.output import eval_js\n",
        "import base64\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "BUFFER_TIME = 5.0\n",
        "\n",
        "# CLAHE (doit matcher la capture dataset WEBCAM)\n",
        "# - Le dataset WEBCAM est enregistré en (200x200 GRAY + CLAHE)\n",
        "# - Donc en live, on applique CLAHE 1 fois sur la frame brute pour obtenir la même représentation.\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "\n",
        "def predict_lbph_stable(face_ready_gray_200):\n",
        "    \"\"\"Prédiction LBPH plus robuste: vote sur variantes + médiane des distances.\n",
        "\n",
        "    Retourne: (name: str, distance: float, is_match: bool)\n",
        "    \"\"\"\n",
        "    variants = [\n",
        "        face_ready_gray_200,\n",
        "        cv2.flip(face_ready_gray_200, 1),\n",
        "        cv2.GaussianBlur(face_ready_gray_200, (3, 3), 0),\n",
        "    ]\n",
        "\n",
        "    preds = []\n",
        "    for v in variants:\n",
        "        label_id, dist = recognizer.predict(v)\n",
        "        preds.append((label_id, float(dist)))\n",
        "\n",
        "    # vote majoritaire\n",
        "    counts = {}\n",
        "    for lid, _ in preds:\n",
        "        counts[lid] = counts.get(lid, 0) + 1\n",
        "    best_label_id = max(counts.items(), key=lambda kv: kv[1])[0]\n",
        "\n",
        "    # distance robuste (médiane des distances du label gagnant)\n",
        "    dists = sorted([d for lid, d in preds if lid == best_label_id])\n",
        "    median_dist = dists[len(dists)//2]\n",
        "\n",
        "    name = label_map.get(best_label_id, \"Inconnu\")\n",
        "\n",
        "    # IMPORTANT: ici on renvoie uniquement la stabilité du vote (2/3), PAS le seuil.\n",
        "    # Le seuil est géré plus bas (adaptatif selon taille du visage).\n",
        "    vote_ok = (counts[best_label_id] >= 2)\n",
        "    return name, median_dist, vote_ok\n",
        "\n",
        "# --- CAMÉRA JS ---\n",
        "js_camera = \"\"\"\n",
        "    if (!document.getElementById('videoElement')) {\n",
        "        var video = document.createElement('video');\n",
        "        video.id = 'videoElement';\n",
        "        video.style.display = 'none';\n",
        "        document.body.appendChild(video);\n",
        "        var canvas = document.createElement('canvas');\n",
        "        canvas.id = 'canvasElement';\n",
        "        canvas.style.display = 'none';\n",
        "        document.body.appendChild(canvas);\n",
        "    }\n",
        "    async function stream_frame() {\n",
        "        var video = document.getElementById('videoElement');\n",
        "        var canvas = document.getElementById('canvasElement');\n",
        "        if (video.paused) {\n",
        "            // Plus de pixels + meilleure compression => LBPH beaucoup plus stable\n",
        "            var stream = await navigator.mediaDevices.getUserMedia({\n",
        "              video: {\n",
        "                width: {ideal: 1280},\n",
        "                height: {ideal: 720},\n",
        "                facingMode: 'user'\n",
        "              }\n",
        "            });\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "        }\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        return canvas.toDataURL('image/jpeg', 0.92);\n",
        "    }\n",
        "\"\"\"\n",
        "display(Javascript(js_camera))\n",
        "\n",
        "def js_to_image(js_reply):\n",
        "    image_bytes = base64.b64decode(js_reply.split(',')[1])\n",
        "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "    img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "    return img\n",
        "\n",
        "def format_time(seconds):\n",
        "    m, s = divmod(int(seconds), 60)\n",
        "    return f\"{m:02d}:{s:02d}\"\n",
        "\n",
        "# --- INITIALISATION SESSION ---\n",
        "# (On s'assure que ELEVES_ATTENDUS existe, sinon liste vide pour test)\n",
        "if 'ELEVES_ATTENDUS' not in globals(): ELEVES_ATTENDUS = []\n",
        "\n",
        "global db_eleves_session, start_time_session, end_time_session\n",
        "db_eleves_session = {\n",
        "    eleve: {'first_seen': None, 'last_seen': None, 'status': 'ABSENT'}\n",
        "    for eleve in ELEVES_ATTENDUS\n",
        "}\n",
        "start_time_session = time.time()\n",
        "end_time_session = None\n",
        "\n",
        "# Chargement du classifieur Haar\n",
        "face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))\n",
        "\n",
        "# --- UI ---\n",
        "header_widget = widgets.HTML(\"<h2>Initialisation...</h2>\")\n",
        "video_widget = widgets.Image(format='jpeg', width=500, height=375)\n",
        "status_widget = widgets.HTML(value=\"\", layout=widgets.Layout(width='400px', height='375px', border='1px solid #ccc', overflow='auto', padding='5px'))\n",
        "\n",
        "print(f\"COURS DÉMARRÉ. Seuil Distance : {SEUIL_DISTANCE}\")\n",
        "display(header_widget)\n",
        "display(widgets.HBox([video_widget, status_widget]))\n",
        "\n",
        "# Seuils: en pratique, quand 2 personnes sont dans le cadre, chaque visage est plus petit\n",
        "# => distances LBPH montent (tu observes ~190 solo, ~200-220 à 2). On adapte le seuil.\n",
        "SEUIL_DISTANCE_BIG_FACE = 205    # visage grand (solo / proche)\n",
        "SEUIL_DISTANCE_SMALL_FACE = 235  # visage plus petit\n",
        "SEUIL_DISTANCE_MULTI_FACE = 245  # quand 2+ visages sont détectés dans la frame\n",
        "MIN_BIG_FACE = 340              # px (min(w,h))\n",
        "\n",
        "# Anti-flap: exige X frames consécutives pour valider un élève\n",
        "CONSECUTIVE_MATCHES_REQUIRED = 3\n",
        "CONSECUTIVE_MATCHES_REQUIRED_SMALL_FACE = 3\n",
        "match_streak = {}\n",
        "\n",
        "# --- BOUCLE PRINCIPALE ---\n",
        "try:\n",
        "    while True:\n",
        "        current_time = time.time()\n",
        "        duree_cours = current_time - start_time_session\n",
        "        header_widget.value = f\"<h2 style='color:#333;'>DURÉE DU COURS : {format_time(duree_cours)}</h2>\"\n",
        "\n",
        "        # 1. Capture Image\n",
        "        js_reply = eval_js('stream_frame()')\n",
        "        frame = js_to_image(js_reply)\n",
        "\n",
        "        # 2. Conversion Gris + Détection\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # IMPORTANT: on veut un visage assez grand et on garde uniquement le + grand\n",
        "        # (sinon LBPH décroche et donne des distances 150-300)\n",
        "        faces = face_cascade.detectMultiScale(\n",
        "            gray,\n",
        "            scaleFactor=1.2,\n",
        "            minNeighbors=5,\n",
        "            minSize=(140, 140)\n",
        "        )\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            faces = sorted(faces, key=lambda f: f[2] * f[3], reverse=True)\n",
        "\n",
        "        num_faces = len(faces)\n",
        "        display_frame = frame.copy()\n",
        "\n",
        "        for (x, y, w, h) in faces:\n",
        "            # 3. Préparation Visage pour LBPH (mêmes idées que le crop dataset)\n",
        "            margin = max(10, int(0.15 * min(w, h)))\n",
        "            x0 = max(0, x - margin)\n",
        "            y0 = max(0, y - margin)\n",
        "            x1 = min(gray.shape[1], x + w + margin)\n",
        "            y1 = min(gray.shape[0], y + h + margin)\n",
        "\n",
        "            face_roi = gray[y0:y1, x0:x1]\n",
        "\n",
        "            face_resized = cv2.resize(face_roi, (200, 200))\n",
        "            # IMPORTANT: CLAHE 1x (pour matcher le dataset WEBCAM)\n",
        "            face_ready = clahe.apply(face_resized)\n",
        "\n",
        "            # 4. Prédiction (plus robuste)\n",
        "            try:\n",
        "                # Seuil adaptatif (2 personnes => visages plus petits => distances plus hautes)\n",
        "                is_big_face = min(w, h) >= MIN_BIG_FACE\n",
        "                seuil = SEUIL_DISTANCE_BIG_FACE if is_big_face else SEUIL_DISTANCE_SMALL_FACE\n",
        "                if num_faces >= 2:\n",
        "                    seuil = max(seuil, SEUIL_DISTANCE_MULTI_FACE)\n",
        "\n",
        "                required = CONSECUTIVE_MATCHES_REQUIRED if is_big_face else CONSECUTIVE_MATCHES_REQUIRED_SMALL_FACE\n",
        "\n",
        "                # Vote: sur petits visages / multi-face, le vote flip/blur peut être instable.\n",
        "                # Donc on accepte le vote même si ce n'est pas 2/3 (vote_ok=1) quand 2+ visages.\n",
        "                detected_name, distance, vote_ok = predict_lbph_stable(face_ready)\n",
        "                if num_faces >= 2:\n",
        "                    vote_ok = True\n",
        "\n",
        "                is_match = vote_ok and (distance < seuil)\n",
        "\n",
        "                # Anti-flap: streak par nom (suffisant pour un système d'appel)\n",
        "\n",
        "                if is_match:\n",
        "                    match_streak[detected_name] = match_streak.get(detected_name, 0) + 1\n",
        "                else:\n",
        "                    match_streak[detected_name] = max(0, match_streak.get(detected_name, 0) - 1)\n",
        "\n",
        "                stable_match = is_match and (match_streak.get(detected_name, 0) >= required)\n",
        "\n",
        "                if stable_match:\n",
        "                    # correspondance valide (stable)\n",
        "                    if detected_name in db_eleves_session:\n",
        "                        color = (0, 255, 0)  # Vert (Présent)\n",
        "                        label_text = f\"{detected_name} ({int(distance)}) {w}x{h}\"\n",
        "\n",
        "                        # Update DB\n",
        "                        db_eleves_session[detected_name]['last_seen'] = current_time\n",
        "                        if db_eleves_session[detected_name]['first_seen'] is None:\n",
        "                            db_eleves_session[detected_name]['first_seen'] = current_time\n",
        "                    else:\n",
        "                        color = (0, 165, 255)  # Orange (Intrus connu)\n",
        "                        label_text = f\"Intrus: {detected_name} ({int(distance)})\"\n",
        "                else:\n",
        "                    # Trop de distance ou pas stable -> Inconnu\n",
        "                    color = (0, 0, 255)  # Rouge\n",
        "                    # debug: meilleur label prédit même si on rejette + taille visage\n",
        "                    label_text = f\"Inconnu ({int(distance)}) best:{detected_name} {w}x{h} seuil:{int(seuil)} votes:{int(vote_ok)} streak:{match_streak.get(detected_name, 0)}/{required}\"\n",
        "\n",
        "            except Exception:\n",
        "                label_text = \"Erreur\"\n",
        "                color = (0, 0, 255)\n",
        "\n",
        "            # Dessin\n",
        "            cv2.rectangle(display_frame, (x, y), (x+w, y+h), color, 2)\n",
        "            cv2.rectangle(display_frame, (x, y-30), (x+w, y), color, -1)\n",
        "            cv2.putText(display_frame, label_text, (x+5, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)\n",
        "\n",
        "        # --- TABLEAU DE BORD ---\n",
        "        html_content = \"<table style='width:100%; font-family:sans-serif; font-size:13px; border-collapse: collapse;'>\"\n",
        "        html_content += \"<tr style='background:#333; color:white;'><th style='padding:5px;'>Élève</th><th>Statut</th><th>Détails</th></tr>\"\n",
        "\n",
        "        for eleve in ELEVES_ATTENDUS:\n",
        "            data = db_eleves_session[eleve]\n",
        "            if data['first_seen'] is None:\n",
        "                status_txt = \"ABSENT\"\n",
        "                style = \"color:red;\"\n",
        "                retard = current_time - start_time_session\n",
        "                timer_txt = f\"{format_time(retard)}\"\n",
        "            else:\n",
        "                time_since_last = current_time - data['last_seen']\n",
        "                retard_arrivee = data['first_seen'] - start_time_session\n",
        "                str_arrivee = f\"Arr: +{format_time(retard_arrivee)}\"\n",
        "\n",
        "                if time_since_last < BUFFER_TIME:\n",
        "                    status_txt = \"PRÉSENT\"\n",
        "                    style = \"color:green; font-weight:bold;\"\n",
        "                    timer_txt = str_arrivee\n",
        "                else:\n",
        "                    status_txt = \"PARTI\"\n",
        "                    style = \"color:orange; font-weight:bold;\"\n",
        "                    timer_txt = f\"Parti: {format_time(time_since_last)}\"\n",
        "\n",
        "            html_content += f\"<tr style='border-bottom:1px solid #ddd; {style}'>\"\n",
        "            html_content += f\"<td style='padding:5px;'>{eleve}</td>\"\n",
        "            html_content += f\"<td style='text-align:center;'>{status_txt}</td>\"\n",
        "            html_content += f\"<td style='text-align:right; font-family:monospace;'>{timer_txt}</td></tr>\"\n",
        "\n",
        "        html_content += \"</table>\"\n",
        "\n",
        "        _, encoded_img = cv2.imencode('.jpg', display_frame)\n",
        "        video_widget.value = encoded_img.tobytes()\n",
        "        status_widget.value = html_content\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    end_time_session = time.time()\n",
        "    try:\n",
        "        from zoneinfo import ZoneInfo\n",
        "        now_local = datetime.now(ZoneInfo(\"Europe/Paris\"))\n",
        "    except Exception:\n",
        "        # fallback: heure système (peut rester UTC sur Colab)\n",
        "        now_local = datetime.now()\n",
        "    print(f\"\\n✅ Cours arrêté. Fin : {now_local.strftime('%H:%M:%S')}\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur Critique : {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LOmCtZc-89f"
      },
      "source": [
        "## 5. Génération du rapport final du cours"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6WShjdF63ts"
      },
      "source": [
        "Une fois le cours fini, toutes les variables sont stockées, on fait les calculs pour le retard, la sortie anticipé et l'absence.\n",
        "On affiche le compte rendu du cours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io_Ke-5UJbU3"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Vérification de sécurité (si on lance cette cellule sans avoir fait le cours)\n",
        "if 'end_time_session' not in globals() or end_time_session is None:\n",
        "    print(\"Attention : Le cours n'a pas été arrêté correctement ou n'a pas commencé.\")\n",
        "    end_time_session = time.time() # Valeur par défaut pour ne pas planter\n",
        "\n",
        "# Calcul sur la base du temps FIGÉ\n",
        "duree_totale_cours = end_time_session - start_time_session\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"RAPPORT DÉTAILLÉ SPOT - Fin du cours enregistrée\")\n",
        "print(f\"⏱Durée VALIDÉE du cours : {format_time(duree_totale_cours)}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "recap_data = []\n",
        "\n",
        "for eleve in ELEVES_ATTENDUS:\n",
        "    data = db_eleves_session[eleve]\n",
        "\n",
        "    retard_sec = 0.0\n",
        "    sortie_sec = 0.0\n",
        "    total_absence_sec = 0.0\n",
        "\n",
        "    col_retard = \"-\"\n",
        "    col_sortie = \"-\"\n",
        "    col_total = \"-\"\n",
        "    statut_final = \"ABSENT\"\n",
        "\n",
        "    # --- CALCULS ---\n",
        "    if data['first_seen'] is None:\n",
        "        # ABSENT TOUT LE LONG\n",
        "        statut_final = \"ABSENT\"\n",
        "        total_absence_sec = duree_totale_cours\n",
        "        col_total = format_time(total_absence_sec)\n",
        "        col_retard = \"Absent tout le cours\"\n",
        "        col_sortie = \"-\"\n",
        "\n",
        "    else:\n",
        "        # VENU AU MOINS UNE FOIS\n",
        "\n",
        "        # 1. Retard Début\n",
        "        retard_sec = data['first_seen'] - start_time_session\n",
        "        if retard_sec > 1.0:\n",
        "            col_retard = f\"+ {format_time(retard_sec)}\"\n",
        "        else:\n",
        "            col_retard = \"À l'heure\"\n",
        "            retard_sec = 0\n",
        "\n",
        "        # 2. Sortie Fin (Basé sur le temps figé end_time_session)\n",
        "        temps_depuis_derniere_vue = end_time_session - data['last_seen']\n",
        "\n",
        "        # Marge technique de 10s\n",
        "        if temps_depuis_derniere_vue < (BUFFER_TIME + 10.0):\n",
        "            statut_final = \"PRÉSENT\"\n",
        "            col_sortie = \"Non (Présent)\"\n",
        "            sortie_sec = 0\n",
        "        else:\n",
        "            statut_final = \"PARTI AVANT LA FIN\"\n",
        "            sortie_sec = temps_depuis_derniere_vue\n",
        "            col_sortie = f\"- {format_time(sortie_sec)}\"\n",
        "\n",
        "        # 3. Total\n",
        "        total_absence_sec = retard_sec + sortie_sec\n",
        "        col_total = format_time(total_absence_sec)\n",
        "\n",
        "    recap_data.append([eleve, statut_final, col_retard, col_sortie, col_total])\n",
        "\n",
        "df = pd.DataFrame(recap_data, columns=[\n",
        "    \"Élève\",\n",
        "    \"Statut Final\",\n",
        "    \"Retard (Début)\",\n",
        "    \"Sortie Anticipée (Fin)\",\n",
        "    \"Total Absence\"\n",
        "])\n",
        "\n",
        "# Styles\n",
        "def color_status(val):\n",
        "    if val == 'PRÉSENT': return 'color: green; font-weight: bold'\n",
        "    if val == 'ABSENT': return 'color: red; font-weight: bold'\n",
        "    if val == 'PARTI AVANT LA FIN': return 'color: orange; font-weight: bold'\n",
        "    return ''\n",
        "\n",
        "styles = [\n",
        "    dict(selector=\"th\", props=[(\"text-align\", \"center\")]),\n",
        "    dict(selector=\"td\", props=[(\"text-align\", \"center\")])\n",
        "]\n",
        "\n",
        "display(df.style.map(color_status, subset=['Statut Final']).set_table_styles(styles))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}